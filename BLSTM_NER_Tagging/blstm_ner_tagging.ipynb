{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7cda42a3",
      "metadata": {
        "id": "7cda42a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab8d311-e418-443f-da65-3eb5e09f833b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import torch.nn.utils.rnn as rnn\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "83acf7ab",
      "metadata": {
        "id": "83acf7ab"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"train\", sep=' ', header=None, quoting=csv.QUOTE_NONE, names=['index', 'word', 'tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6e20d845",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e20d845",
        "outputId": "431bc32b-17b3-4979-93b0-bc3f93dbaa9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23624\n"
          ]
        }
      ],
      "source": [
        "vocabulary = {}\n",
        "def create_vocab(word, vocabulary):\n",
        "    if word in vocabulary:\n",
        "        vocabulary[word] += 1\n",
        "    else:\n",
        "        vocabulary[word] = 1 \n",
        "    return word\n",
        "\n",
        "train['word_d'] = train['word'].apply(lambda x: create_vocab(x, vocabulary))\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "riNF-qiVmXY7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riNF-qiVmXY7",
        "outputId": "da992e19-c98d-4d86-c1fb-84e633b384f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'B-ORG', 1: 'O', 2: 'B-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-LOC', 6: 'I-ORG', 7: 'I-MISC', 8: 'I-LOC', -1: 'PAD'}\n"
          ]
        }
      ],
      "source": [
        "tag2idx = {}\n",
        "\n",
        "def create_tagset(tag, tagset, ):\n",
        "  if tag not in tagset:\n",
        "    l = len(tagset)\n",
        "    tagset[tag] = l\n",
        "\n",
        "\n",
        "train['tag_d'] = train['tag'].apply(lambda x: create_tagset(x, tag2idx))\n",
        "tag2idx['PAD'] = -1\n",
        "\n",
        "idx2tag = {value:key for key, value in tag2idx.items()}\n",
        "print(idx2tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9946f0e1",
      "metadata": {
        "id": "9946f0e1"
      },
      "outputs": [],
      "source": [
        "def filter_vocab(vocabulary, threshold):\n",
        "    filtered_vocabulary = {}\n",
        "\n",
        "    i = 2\n",
        "    for word, value in vocabulary.items():\n",
        "        if value > threshold:\n",
        "            filtered_vocabulary[word] = i\n",
        "            i += 1\n",
        "\n",
        "    filtered_vocabulary['_pad'] = 0\n",
        "    filtered_vocabulary['_unk'] = 1\n",
        "    return filtered_vocabulary\n",
        "            \n",
        "threshold = 2\n",
        "word2idx = filter_vocab(vocabulary, threshold) \n",
        "idx2word = {value:key for key, value in word2idx.items()}  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "_LrRcmXMozQ_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LrRcmXMozQ_",
        "outputId": "af391e46-5faf-4135-932f-bbe7baf18f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8129\n",
            "8129\n"
          ]
        }
      ],
      "source": [
        "print(len(word2idx))\n",
        "print(len(idx2word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5f7760f9",
      "metadata": {
        "id": "5f7760f9"
      },
      "outputs": [],
      "source": [
        "from pandas.core.frame import validate_axis_style_args\n",
        "\n",
        "# prepare sequences\n",
        "\n",
        "train_indices = train['index'].values\n",
        "train_words = train['word'].values\n",
        "train_tags = train['tag'].values\n",
        "\n",
        "def pad_sentence(seq, id):\n",
        "  padsize = 113 - len(seq)\n",
        "  if padsize == 113:\n",
        "    return np.array(seq)\n",
        "  if id == \"word\":\n",
        "    return np.concatenate((np.array(seq), np.zeros(padsize)))\n",
        "  else:\n",
        "    return np.concatenate((np.array(seq), -1*np.ones(padsize)))\n",
        "  \n",
        "\n",
        "def prepare_sequences(indices, words, tags):\n",
        "  word_sequences = []\n",
        "  tag_sequences = []\n",
        "  word_seq = []\n",
        "  tag_seq = []\n",
        "  lengths = []\n",
        "  \n",
        "  count = 0\n",
        "  for i in range(len(indices)):\n",
        "    if indices[i] == 1:\n",
        "      count += 1\n",
        "      if i != 0:\n",
        "        lengths.append(len(word_seq))\n",
        "        word_sequences.append(pad_sentence(word_seq,\"word\"))\n",
        "        tag_sequences.append(pad_sentence(tag_seq,\"tag\"))\n",
        "     \n",
        "      word_seq = []\n",
        "      tag_seq = []\n",
        "\n",
        "    word = \"_unk\"\n",
        "    if words[i] in word2idx:\n",
        "      word = words[i]\n",
        "    word_seq.append(word2idx[word])\n",
        "    tag_seq.append(tag2idx[tags[i]])\n",
        "\n",
        "  lengths.append(len(word_seq))\n",
        "  word_sequences.append(pad_sentence(word_seq,\"word\"))\n",
        "  tag_sequences.append(pad_sentence(tag_seq,\"tag\"))\n",
        "  return np.array(word_sequences), np.array(tag_sequences), lengths\n",
        "\n",
        "train_x, train_y, train_lengths = prepare_sequences(train_indices, train_words, train_tags)\n",
        "\n",
        "dev = pd.read_csv(\"dev\", sep=' ', header=None, quoting=csv.QUOTE_NONE, names=['index', 'word', 'tag'])\n",
        "dev_indices = dev['index'].values\n",
        "dev_words = dev['word'].values\n",
        "dev_tags = dev['tag'].values\n",
        "\n",
        "dev_x, dev_y, dev_lengths = prepare_sequences(dev_indices, dev_words, dev_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8EFz4RSjari8",
      "metadata": {
        "id": "8EFz4RSjari8"
      },
      "outputs": [],
      "source": [
        "class TrainData(Dataset):\n",
        "    def __init__(self, train_tensors, train_labels, train_lengths):\n",
        "        self.data = torch.from_numpy(train_tensors)\n",
        "        self.data = (self.data).to(torch.int64)\n",
        "        self.labels = torch.from_numpy(train_labels)\n",
        "        self.labels = (self.labels).to(torch.int64)\n",
        "        self.lengths = train_lengths\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.lengths[index], self.labels[index]\n",
        "\n",
        "class ValData(Dataset):\n",
        "    def __init__(self, val_tensors, val_labels, val_lengths):\n",
        "        self.data = torch.from_numpy(val_tensors)\n",
        "        self.data = (self.data).to(torch.int64)\n",
        "        self.labels = torch.from_numpy(val_labels)\n",
        "        self.labels = (self.labels).to(torch.int64)\n",
        "        self.lengths = val_lengths\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.lengths[index], self.labels[index]\n",
        "    \n",
        "class TestData(Dataset):\n",
        "    def __init__(self, test_tensors, test_labels, test_lengths):\n",
        "        self.data = torch.from_numpy(test_tensors)\n",
        "        self.data = (self.data).to(torch.int64)\n",
        "        self.labels = torch.from_numpy(test_labels)\n",
        "        self.labels = (self.labels).to(torch.int64)\n",
        "        self.lengths = test_lengths\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.lengths[index], self.labels[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "JSWYFZveoi1d",
      "metadata": {
        "id": "JSWYFZveoi1d"
      },
      "outputs": [],
      "source": [
        "class BLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, linear_output_dim, output_dim, embedding_dim, hidden_dim, dropout_prob):\n",
        "        super(BLSTM_NER, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
        "        self.linear1 = nn.Linear(2*hidden_dim, linear_output_dim)\n",
        "        self.linear2 = nn.Linear(linear_output_dim, output_dim)\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, sentence, lengths):\n",
        "        embeddings = self.embeddings(sentence)\n",
        "        lstm_out = rnn.pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.blstm(lstm_out)\n",
        "        lstm_out, _ = rnn.pad_packed_sequence(lstm_out, batch_first=True, padding_value=0.0, total_length=sentence.shape[1])\n",
        "        lstm_out = self.linear1(lstm_out)\n",
        "        lstm_out = self.elu(lstm_out)\n",
        "\n",
        "        output = self.linear2(lstm_out)\n",
        "        tag_scores = f.log_softmax(output, dim=1)\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING DATA\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(TrainData(train_x, train_y, train_lengths), batch_size=batch_size)\n",
        "val_loader = torch.utils.data.DataLoader(ValData(dev_x, dev_y, dev_lengths), batch_size=10)"
      ],
      "metadata": {
        "id": "8yh7p9IokDr2"
      },
      "id": "8yh7p9IokDr2",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yauJWeDpoJIP",
      "metadata": {
        "id": "yauJWeDpoJIP"
      },
      "outputs": [],
      "source": [
        "# vocab_size = len(word2idx)\n",
        "# embedding_dim = 100\n",
        "# hidden_dim = 256\n",
        "# dropout_prob = 0.33\n",
        "# linear_output_dim = 128\n",
        "# output_dim = 9\n",
        "\n",
        "# model = BLSTM_NER(23626, linear_output_dim, output_dim, embedding_dim, hidden_dim, dropout_prob).to(device)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss( ignore_index = -1)\n",
        "# if torch.cuda.is_available():\n",
        "#   criterion = nn.CrossEntropyLoss( ignore_index = -1).cuda()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "# # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "# # scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "# # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=0.05, last_epoch=-1, verbose=False)\n",
        "# batch_size = 20\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(TrainData(train_x, train_y, train_lengths), batch_size=batch_size)\n",
        "# val_loader = torch.utils.data.DataLoader(ValData(dev_x, dev_y, dev_lengths), batch_size=10)\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def task1(train_loader, val_loader):\n",
        "\n",
        "  # HYPERPARAMS -------------------------------------------------------------------------------\n",
        "  vocab_size = len(word2idx)\n",
        "  embedding_dim = 100\n",
        "  hidden_dim = 256\n",
        "  dropout_prob = 0.33\n",
        "  linear_output_dim = 128\n",
        "  output_dim = 9\n",
        "\n",
        "  model = BLSTM_NER(11985, linear_output_dim, output_dim, embedding_dim, hidden_dim, dropout_prob).to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss( ignore_index = -1)\n",
        "  if torch.cuda.is_available():\n",
        "    criterion = nn.CrossEntropyLoss( ignore_index = -1).cuda()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "  # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "  # scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=0.05, last_epoch=-1, verbose=False)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.93, steps_per_epoch=len(train_loader), epochs=50)\n",
        "\n",
        "  epochs = 50\n",
        "  counter = 0\n",
        "  print_every = 1000\n",
        "  clip = 5\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  model.train()\n",
        "  for i in range(epochs):\n",
        "\n",
        "      for data, lengths, targets in train_loader:\n",
        "          counter += 1\n",
        "          data = data.to(device)\n",
        "          \n",
        "          targets = targets.to(device)\n",
        "          # h = tuple([e.data for e in h])\n",
        "          # inputs, labels = inputs.to(device), labels.to(device)\n",
        "          model.zero_grad()\n",
        "          output = model(data.to(device), lengths)\n",
        "\n",
        "          y_pred_for_loss = output.permute(0,2,1)\n",
        "          y_pred_for_loss = y_pred_for_loss.to(device)\n",
        "          \n",
        "          loss = criterion(y_pred_for_loss, targets)\n",
        "          loss.backward()\n",
        "          nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "          optimizer.step()\n",
        "        \n",
        "          # if counter%print_every == 0:\n",
        "\n",
        "          #   val_losses = []\n",
        "          #   model.eval()\n",
        "          #   for val_data, val_lengths, val_targets in val_loader:\n",
        "                \n",
        "          #       val_data = val_data.to(device)\n",
        "                \n",
        "          #       val_targets = val_targets.to(device)\n",
        "          #       val_output = model(val_data, val_lengths)\n",
        "          #       val_pred_for_loss = val_output.permute(0,2,1)\n",
        "          #       val_pred_for_loss = val_pred_for_loss.to(device)\n",
        "          #       val_loss = criterion(val_pred_for_loss, val_targets)\n",
        "          #       val_losses.append(val_loss.item())\n",
        "                \n",
        "          #   model.train()\n",
        "          #   print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "          #         \"Step: {}...\".format(counter),\n",
        "          #         \"Loss: {:.6f}...\".format(loss.item()),\n",
        "          #         \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "          #   # if np.mean(val_losses) <= valid_loss_min:\n",
        "          #   #     torch.save(model.state_dict(), './state_dict.pt')\n",
        "          #   #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "          #   #     valid_loss_min = np.mean(val_losses)\n",
        "          scheduler.step()\n",
        "\n",
        "  return model\n",
        "\n",
        "task1model = task1(train_loader, val_loader)\n"
      ],
      "metadata": {
        "id": "2oshh6MRjo72"
      },
      "id": "2oshh6MRjo72",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "B0gMyoY1TzqP",
      "metadata": {
        "id": "B0gMyoY1TzqP"
      },
      "outputs": [],
      "source": [
        "# GET DEV RESULTS\n",
        "task1model.eval()\n",
        "\n",
        "dev_x_tensor = torch.from_numpy(dev_x)\n",
        "dev_x_tensor = dev_x_tensor.to(torch.int64)\n",
        "dev_y_tensor = torch.from_numpy(dev_y)\n",
        "dev_y_tensor = dev_y_tensor.to(torch.int64)\n",
        "\n",
        "dev_output = task1model(dev_x_tensor.to(device), dev_lengths)\n",
        "dev_output = dev_output.cpu().detach().numpy()\n",
        "\n",
        "# FUNCTIONS FOR GETTING PREDICTIONS\n",
        "def get_preds(output, lengths):\n",
        "  preds = []\n",
        "  for i in range(output.shape[0]):\n",
        "      tmp = []\n",
        "      \n",
        "      for j in range(lengths[i]):\n",
        "          tmp.append(np.argmax(output[i][j]))\n",
        "          \n",
        "      preds.append(tmp)\n",
        "  return preds\n",
        "\n",
        "def write_predictions(filename, indices, words, tags, preds, idx2word, idx2tag):\n",
        "    \n",
        "    count = 0\n",
        "    file = open(filename, \"w\")\n",
        "    \n",
        "    for i in range(len(preds)):\n",
        "        if i != 0:\n",
        "            file.write(\"\\n\")\n",
        "        for j in range(len(preds[i])):\n",
        "            s = str(indices[count]) + \" \" + str(words[count]) + \" \" + str(tags[count]) + \" \" + str(idx2tag[preds[i][j]]) + \"\\n\"\n",
        "            file.write(s)\n",
        "            count += 1\n",
        "            \n",
        "    file.close()\n",
        "    return\n",
        "\n",
        "\n",
        "# GETTING PREDICTIONS ON DEV SET\n",
        "dev_preds = get_preds(dev_output, dev_lengths)\n",
        "write_predictions(\"dev1pred.out\", dev_indices, dev_words, dev_tags, dev_preds, idx2word, idx2tag)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE MODEL\n",
        "\n",
        "task1model.train()\n",
        "torch.save(task1model.state_dict(), './blstm1.pt')"
      ],
      "metadata": {
        "id": "PBlRYeOi7xv3"
      },
      "id": "PBlRYeOi7xv3",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: "
      ],
      "metadata": {
        "id": "BYMGZd_BNMN6"
      },
      "id": "BYMGZd_BNMN6"
    },
    {
      "cell_type": "code",
      "source": [
        "# READ GLOVE EMBEDDINGS\n",
        "\n",
        "glove = pd.read_csv(\"glove.6B.100d\", sep=' ', header=None, quoting=csv.QUOTE_NONE)\n",
        "glove.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "SUozJ6S3mAzR",
        "outputId": "690c4566-b8ae-4383-b7da-44dbb0323dd6"
      },
      "id": "SUozJ6S3mAzR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0         1         2        3         4         5         6         7    \\\n",
              "0  the -0.038194 -0.244870  0.72812 -0.399610  0.083172  0.043953 -0.391410   \n",
              "1    , -0.107670  0.110530  0.59812 -0.543610  0.673960  0.106630  0.038867   \n",
              "2    . -0.339790  0.209410  0.46348 -0.647920 -0.383770  0.038034  0.171270   \n",
              "3   of -0.152900 -0.242790  0.89837  0.169960  0.535160  0.487840 -0.588260   \n",
              "4   to -0.189700  0.050024  0.19084 -0.049184 -0.089737  0.210060 -0.549520   \n",
              "\n",
              "        8        9    ...       91        92        93       94       95   \\\n",
              "0  0.334400 -0.57545  ...  0.016215 -0.017099 -0.389840  0.87424 -0.72569   \n",
              "1  0.354810  0.06351  ...  0.349510 -0.722600  0.375490  0.44410 -0.99059   \n",
              "2  0.159780  0.46619  ... -0.063351 -0.674120 -0.068895  0.53604 -0.87773   \n",
              "3 -0.179820 -1.35810  ...  0.187120 -0.018488 -0.267570  0.72700 -0.59363   \n",
              "4  0.098377 -0.20135  ... -0.131340  0.058617 -0.318690 -0.61419 -0.62393   \n",
              "\n",
              "       96        97       98       99        100  \n",
              "0 -0.51058 -0.520280 -0.14590  0.82780  0.270620  \n",
              "1  0.61214 -0.351110 -0.83155  0.45293  0.082577  \n",
              "2  0.31802 -0.392420 -0.23394  0.47298 -0.028803  \n",
              "3 -0.34839 -0.560940 -0.59100  1.00390  0.206640  \n",
              "4 -0.41548 -0.038175 -0.39804  0.47647 -0.159830  \n",
              "\n",
              "[5 rows x 101 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b678c2e4-2774-44dc-aa2a-a9f4b5e9a39b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>-0.038194</td>\n",
              "      <td>-0.244870</td>\n",
              "      <td>0.72812</td>\n",
              "      <td>-0.399610</td>\n",
              "      <td>0.083172</td>\n",
              "      <td>0.043953</td>\n",
              "      <td>-0.391410</td>\n",
              "      <td>0.334400</td>\n",
              "      <td>-0.57545</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016215</td>\n",
              "      <td>-0.017099</td>\n",
              "      <td>-0.389840</td>\n",
              "      <td>0.87424</td>\n",
              "      <td>-0.72569</td>\n",
              "      <td>-0.51058</td>\n",
              "      <td>-0.520280</td>\n",
              "      <td>-0.14590</td>\n",
              "      <td>0.82780</td>\n",
              "      <td>0.270620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>,</td>\n",
              "      <td>-0.107670</td>\n",
              "      <td>0.110530</td>\n",
              "      <td>0.59812</td>\n",
              "      <td>-0.543610</td>\n",
              "      <td>0.673960</td>\n",
              "      <td>0.106630</td>\n",
              "      <td>0.038867</td>\n",
              "      <td>0.354810</td>\n",
              "      <td>0.06351</td>\n",
              "      <td>...</td>\n",
              "      <td>0.349510</td>\n",
              "      <td>-0.722600</td>\n",
              "      <td>0.375490</td>\n",
              "      <td>0.44410</td>\n",
              "      <td>-0.99059</td>\n",
              "      <td>0.61214</td>\n",
              "      <td>-0.351110</td>\n",
              "      <td>-0.83155</td>\n",
              "      <td>0.45293</td>\n",
              "      <td>0.082577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.</td>\n",
              "      <td>-0.339790</td>\n",
              "      <td>0.209410</td>\n",
              "      <td>0.46348</td>\n",
              "      <td>-0.647920</td>\n",
              "      <td>-0.383770</td>\n",
              "      <td>0.038034</td>\n",
              "      <td>0.171270</td>\n",
              "      <td>0.159780</td>\n",
              "      <td>0.46619</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063351</td>\n",
              "      <td>-0.674120</td>\n",
              "      <td>-0.068895</td>\n",
              "      <td>0.53604</td>\n",
              "      <td>-0.87773</td>\n",
              "      <td>0.31802</td>\n",
              "      <td>-0.392420</td>\n",
              "      <td>-0.23394</td>\n",
              "      <td>0.47298</td>\n",
              "      <td>-0.028803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of</td>\n",
              "      <td>-0.152900</td>\n",
              "      <td>-0.242790</td>\n",
              "      <td>0.89837</td>\n",
              "      <td>0.169960</td>\n",
              "      <td>0.535160</td>\n",
              "      <td>0.487840</td>\n",
              "      <td>-0.588260</td>\n",
              "      <td>-0.179820</td>\n",
              "      <td>-1.35810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.187120</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>-0.267570</td>\n",
              "      <td>0.72700</td>\n",
              "      <td>-0.59363</td>\n",
              "      <td>-0.34839</td>\n",
              "      <td>-0.560940</td>\n",
              "      <td>-0.59100</td>\n",
              "      <td>1.00390</td>\n",
              "      <td>0.206640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>to</td>\n",
              "      <td>-0.189700</td>\n",
              "      <td>0.050024</td>\n",
              "      <td>0.19084</td>\n",
              "      <td>-0.049184</td>\n",
              "      <td>-0.089737</td>\n",
              "      <td>0.210060</td>\n",
              "      <td>-0.549520</td>\n",
              "      <td>0.098377</td>\n",
              "      <td>-0.20135</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.131340</td>\n",
              "      <td>0.058617</td>\n",
              "      <td>-0.318690</td>\n",
              "      <td>-0.61419</td>\n",
              "      <td>-0.62393</td>\n",
              "      <td>-0.41548</td>\n",
              "      <td>-0.038175</td>\n",
              "      <td>-0.39804</td>\n",
              "      <td>0.47647</td>\n",
              "      <td>-0.159830</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b678c2e4-2774-44dc-aa2a-a9f4b5e9a39b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b678c2e4-2774-44dc-aa2a-a9f4b5e9a39b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b678c2e4-2774-44dc-aa2a-a9f4b5e9a39b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vocab = glove[0].values\n",
        "print(glove_vocab.shape)\n",
        "glove_embeddings = glove.iloc[:, 1:]\n",
        "print(glove_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KbF3ad-sl4h",
        "outputId": "9c8c45dc-6f34-454b-b9e9-08948ab9848c"
      },
      "id": "0KbF3ad-sl4h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(400000,)\n",
            "(400000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_embeddings = np.array(glove_embeddings)\n",
        "_unk_embedding = np.mean(glove_embeddings,axis=0,keepdims=True)\n",
        "print(_unk_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLLBiOw5ssD4",
        "outputId": "99761c74-b27f-4dc3-8b71-cb68da7b1117"
      },
      "id": "tLLBiOw5ssD4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_glove_vocab_dict(glove_vocab, glove_embeddings):\n",
        "  glove_vocab_dict = {}\n",
        "  for i in range(glove_vocab.shape[0]):\n",
        "      glove_vocab_dict[glove_vocab[i]] = glove_embeddings[i]\n",
        "  \n",
        "  return glove_vocab_dict\n",
        "\n",
        "glove_vocab_dict = create_glove_vocab_dict(glove_vocab, glove_embeddings) "
      ],
      "metadata": {
        "id": "4ZWn4YlHJPT9"
      },
      "id": "4ZWn4YlHJPT9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(glove_vocab_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NypGgi2ZtD6j",
        "outputId": "96d27374-32a6-4984-d3c7-d3a7e3be9e68"
      },
      "id": "NypGgi2ZtD6j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "399998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_glove_embeddings(word2idx, glove_vocab_dict, _unk_embedding):\n",
        "  embeddings = np.zeros((1,101))\n",
        "\n",
        "  _pad_embedding = np.zeros((1,101))   #embedding for '<pad>' token.\n",
        "  unk_embed_zero = np.concatenate((_unk_embedding.flatten(), np.zeros(1)))\n",
        "  # unk_embed_one = np.concatenate((_unk_embedding.flatten(), np.ones(1)))\n",
        "\n",
        "  padcount = 0\n",
        "  unkcount = 0\n",
        "  inglove = 0\n",
        "  lowercase = 0\n",
        "  notinglove = 0\n",
        "\n",
        "  embeddings = np.vstack((embeddings,_pad_embedding))\n",
        "  embeddings = np.vstack((embeddings,unk_embed_zero))\n",
        "\n",
        "  for word, idx in word2idx.items():\n",
        "    word = str(word)\n",
        "    if word == \"_pad\":\n",
        "      padcount += 1\n",
        "    elif word == \"_unk\":\n",
        "      unkcount += 1\n",
        "    else:\n",
        "      if word.lower() in glove_vocab_dict:\n",
        "        inglove += 1\n",
        "        if word == word.lower():\n",
        "          embedding = np.concatenate((glove_vocab_dict[word.lower()], np.ones(1)))\n",
        "          embeddings = np.vstack((embeddings,embedding))\n",
        "          lowercase += 1\n",
        "        else:\n",
        "          embedding = np.concatenate((glove_vocab_dict[word.lower()], np.zeros(1)))\n",
        "          embeddings = np.vstack((embeddings,embedding))\n",
        "        \n",
        "      else:\n",
        "        notinglove += 1\n",
        "        if word == word.lower():\n",
        "          embedding = np.concatenate((np.random.rand(100), np.ones(1)))\n",
        "          embeddings = np.vstack((embeddings,embedding))\n",
        "        else:\n",
        "          embedding = np.concatenate((np.random.rand(100), np.zeros(1)))\n",
        "          embeddings = np.vstack((embeddings,embedding))\n",
        "        \n",
        "        \n",
        "\n",
        "  print(\"padcount \", padcount)\n",
        "  print(\"unkcount \", unkcount)\n",
        "  print(\"inglove \", inglove)\n",
        "  print(\"lowercase \", lowercase)\n",
        "  print(\"notinglove \", notinglove)\n",
        "\n",
        "  return embeddings\n",
        "\n",
        "train_embeddings = create_glove_embeddings(word2idx, glove_vocab_dict, _unk_embedding)\n",
        "train_embeddings = train_embeddings[1:]"
      ],
      "metadata": {
        "id": "QMcnXIrRoVJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d97d717-6ad3-4dac-f143-30263d67e8e7"
      },
      "id": "QMcnXIrRoVJo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padcount  1\n",
            "unkcount  1\n",
            "inglove  11510\n",
            "lowercase  6544\n",
            "notinglove  473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai_0OWy3t93z",
        "outputId": "f7979a3f-8e55-4054-b93f-63923353bcf5"
      },
      "id": "Ai_0OWy3t93z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11985, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BLSTM_NER_GLOVE(nn.Module):\n",
        "    def __init__(self, train_embeddings, embedding_dim, linear_output_dim, output_dim, hidden_dim, dropout_prob):\n",
        "        super(BLSTM_NER_GLOVE, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embeddings = nn.Embedding.from_pretrained(torch.from_numpy(train_embeddings).float())\n",
        "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
        "        self.linear1 = nn.Linear(2*hidden_dim, linear_output_dim)\n",
        "        self.linear2 = nn.Linear(linear_output_dim, output_dim)\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, sentence, lengths):\n",
        "        embeddings = self.embeddings(sentence)\n",
        "        lstm_out = rnn.pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.blstm(lstm_out)\n",
        "        lstm_out, _ = rnn.pad_packed_sequence(lstm_out, batch_first=True, padding_value=0.0, total_length=sentence.shape[1])\n",
        "        lstm_out = self.linear1(lstm_out)\n",
        "        lstm_out = self.elu(lstm_out)\n",
        "        output = self.linear2(lstm_out)\n",
        "        tag_scores = f.log_softmax(output, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "2VAk7THGc-8h"
      },
      "id": "2VAk7THGc-8h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def task2(train_loader, val_loader, train_embeddings):\n",
        "\n",
        "  # HYPERPARAMS -------------------------------------------------------------------------------\n",
        "  embedding_dim = 101\n",
        "  hidden_dim = 256\n",
        "  dropout_prob = 0.33\n",
        "  linear_output_dim = 128\n",
        "  output_dim = 9\n",
        "\n",
        "  model = BLSTM_NER_GLOVE(train_embeddings, embedding_dim, linear_output_dim, output_dim, hidden_dim, dropout_prob).to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss( ignore_index = -1)\n",
        "  if torch.cuda.is_available():\n",
        "    criterion = nn.CrossEntropyLoss( ignore_index = -1).cuda()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "  # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "  # scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "  # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=0.05, last_epoch=-1, verbose=False)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1, steps_per_epoch=len(train_loader), epochs=100)\n",
        "\n",
        "  epochs = 100\n",
        "  counter = 0\n",
        "  print_every = 1000\n",
        "  clip = 5\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  model.train()\n",
        "  for i in range(epochs):\n",
        "\n",
        "      for data, lengths, targets in train_loader:\n",
        "          counter += 1\n",
        "          data = data.to(device)\n",
        "          \n",
        "          targets = targets.to(device)\n",
        "          # h = tuple([e.data for e in h])\n",
        "          # inputs, labels = inputs.to(device), labels.to(device)\n",
        "          model.zero_grad()\n",
        "          output = model(data.to(device), lengths)\n",
        "\n",
        "          y_pred_for_loss = output.permute(0,2,1)\n",
        "          y_pred_for_loss = y_pred_for_loss.to(device)\n",
        "          \n",
        "          loss = criterion(y_pred_for_loss, targets)\n",
        "          loss.backward()\n",
        "          nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "          optimizer.step()\n",
        "        \n",
        "          if counter%print_every == 0:\n",
        "\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for val_data, val_lengths, val_targets in val_loader:\n",
        "                \n",
        "                val_data = val_data.to(device)\n",
        "                \n",
        "                val_targets = val_targets.to(device)\n",
        "                val_output = model(val_data, val_lengths)\n",
        "                val_pred_for_loss = val_output.permute(0,2,1)\n",
        "                val_pred_for_loss = val_pred_for_loss.to(device)\n",
        "                val_loss = criterion(val_pred_for_loss, val_targets)\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "            # if np.mean(val_losses) <= valid_loss_min:\n",
        "            #     torch.save(model.state_dict(), './state_dict.pt')\n",
        "            #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "            #     valid_loss_min = np.mean(val_losses)\n",
        "          scheduler.step()\n",
        "\n",
        "  return model\n",
        "\n",
        "task2model = task2(train_loader, val_loader, train_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpJZVPFBq-v5",
        "outputId": "ad50de2f-1873-486e-8288-c65759eb4aea"
      },
      "id": "mpJZVPFBq-v5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/100... Step: 1000... Loss: 0.071471... Val Loss: 0.239023\n",
            "Epoch: 3/100... Step: 2000... Loss: 0.185015... Val Loss: 0.224442\n",
            "Epoch: 4/100... Step: 3000... Loss: 0.004551... Val Loss: 0.149174\n",
            "Epoch: 6/100... Step: 4000... Loss: 0.011556... Val Loss: 0.157440\n",
            "Epoch: 7/100... Step: 5000... Loss: 0.169870... Val Loss: 0.215603\n",
            "Epoch: 8/100... Step: 6000... Loss: 0.002674... Val Loss: 0.136303\n",
            "Epoch: 10/100... Step: 7000... Loss: 0.004220... Val Loss: 0.142497\n",
            "Epoch: 11/100... Step: 8000... Loss: 0.146224... Val Loss: 0.180068\n",
            "Epoch: 12/100... Step: 9000... Loss: 0.075276... Val Loss: 0.132916\n",
            "Epoch: 14/100... Step: 10000... Loss: 0.000979... Val Loss: 0.135869\n",
            "Epoch: 15/100... Step: 11000... Loss: 0.114062... Val Loss: 0.160993\n",
            "Epoch: 16/100... Step: 12000... Loss: 0.000360... Val Loss: 0.136253\n",
            "Epoch: 18/100... Step: 13000... Loss: 0.003181... Val Loss: 0.151762\n",
            "Epoch: 19/100... Step: 14000... Loss: 0.131702... Val Loss: 0.195173\n",
            "Epoch: 20/100... Step: 15000... Loss: 0.000027... Val Loss: 0.149723\n",
            "Epoch: 22/100... Step: 16000... Loss: 0.004729... Val Loss: 0.154854\n",
            "Epoch: 23/100... Step: 17000... Loss: 0.053941... Val Loss: 0.210571\n",
            "Epoch: 24/100... Step: 18000... Loss: 0.000033... Val Loss: 0.175193\n",
            "Epoch: 26/100... Step: 19000... Loss: 0.000730... Val Loss: 0.188053\n",
            "Epoch: 27/100... Step: 20000... Loss: 0.124097... Val Loss: 0.211840\n",
            "Epoch: 28/100... Step: 21000... Loss: 0.000231... Val Loss: 0.242089\n",
            "Epoch: 30/100... Step: 22000... Loss: 0.000065... Val Loss: 0.211325\n",
            "Epoch: 31/100... Step: 23000... Loss: 0.057495... Val Loss: 0.257979\n",
            "Epoch: 32/100... Step: 24000... Loss: 0.000001... Val Loss: 0.248572\n",
            "Epoch: 34/100... Step: 25000... Loss: 0.000594... Val Loss: 0.252645\n",
            "Epoch: 35/100... Step: 26000... Loss: 0.009847... Val Loss: 0.233925\n",
            "Epoch: 36/100... Step: 27000... Loss: 0.000007... Val Loss: 0.256077\n",
            "Epoch: 38/100... Step: 28000... Loss: 0.000163... Val Loss: 0.226992\n",
            "Epoch: 39/100... Step: 29000... Loss: 0.011344... Val Loss: 0.242025\n",
            "Epoch: 40/100... Step: 30000... Loss: 0.000000... Val Loss: 0.283561\n",
            "Epoch: 42/100... Step: 31000... Loss: 0.000003... Val Loss: 0.273222\n",
            "Epoch: 43/100... Step: 32000... Loss: 0.023804... Val Loss: 0.248987\n",
            "Epoch: 44/100... Step: 33000... Loss: 0.000039... Val Loss: 0.252036\n",
            "Epoch: 46/100... Step: 34000... Loss: 0.000000... Val Loss: 0.302300\n",
            "Epoch: 47/100... Step: 35000... Loss: 0.001232... Val Loss: 0.294469\n",
            "Epoch: 48/100... Step: 36000... Loss: 0.000000... Val Loss: 0.313493\n",
            "Epoch: 50/100... Step: 37000... Loss: 0.000000... Val Loss: 0.313496\n",
            "Epoch: 51/100... Step: 38000... Loss: 0.000079... Val Loss: 0.321970\n",
            "Epoch: 52/100... Step: 39000... Loss: 0.000000... Val Loss: 0.328195\n",
            "Epoch: 54/100... Step: 40000... Loss: 0.000000... Val Loss: 0.326846\n",
            "Epoch: 55/100... Step: 41000... Loss: 0.000055... Val Loss: 0.334634\n",
            "Epoch: 56/100... Step: 42000... Loss: 0.000000... Val Loss: 0.337146\n",
            "Epoch: 58/100... Step: 43000... Loss: 0.000000... Val Loss: 0.336363\n",
            "Epoch: 59/100... Step: 44000... Loss: 0.000044... Val Loss: 0.340385\n",
            "Epoch: 60/100... Step: 45000... Loss: 0.000000... Val Loss: 0.347592\n",
            "Epoch: 62/100... Step: 46000... Loss: 0.000000... Val Loss: 0.346091\n",
            "Epoch: 63/100... Step: 47000... Loss: 0.000039... Val Loss: 0.349160\n",
            "Epoch: 64/100... Step: 48000... Loss: 0.000000... Val Loss: 0.355177\n",
            "Epoch: 66/100... Step: 49000... Loss: 0.000000... Val Loss: 0.353837\n",
            "Epoch: 67/100... Step: 50000... Loss: 0.000040... Val Loss: 0.356080\n",
            "Epoch: 68/100... Step: 51000... Loss: 0.000000... Val Loss: 0.360958\n",
            "Epoch: 70/100... Step: 52000... Loss: 0.000000... Val Loss: 0.359780\n",
            "Epoch: 71/100... Step: 53000... Loss: 0.000031... Val Loss: 0.361481\n",
            "Epoch: 72/100... Step: 54000... Loss: 0.000000... Val Loss: 0.365255\n",
            "Epoch: 74/100... Step: 55000... Loss: 0.000000... Val Loss: 0.364699\n",
            "Epoch: 75/100... Step: 56000... Loss: 0.000027... Val Loss: 0.366395\n",
            "Epoch: 76/100... Step: 57000... Loss: 0.000000... Val Loss: 0.369817\n",
            "Epoch: 78/100... Step: 58000... Loss: 0.000000... Val Loss: 0.369233\n",
            "Epoch: 79/100... Step: 59000... Loss: 0.000025... Val Loss: 0.370692\n",
            "Epoch: 80/100... Step: 60000... Loss: 0.000000... Val Loss: 0.373273\n",
            "Epoch: 82/100... Step: 61000... Loss: 0.000000... Val Loss: 0.373230\n",
            "Epoch: 83/100... Step: 62000... Loss: 0.000022... Val Loss: 0.374299\n",
            "Epoch: 84/100... Step: 63000... Loss: 0.000000... Val Loss: 0.375837\n",
            "Epoch: 86/100... Step: 64000... Loss: 0.000000... Val Loss: 0.376337\n",
            "Epoch: 87/100... Step: 65000... Loss: 0.000020... Val Loss: 0.377126\n",
            "Epoch: 88/100... Step: 66000... Loss: 0.000000... Val Loss: 0.378165\n",
            "Epoch: 90/100... Step: 67000... Loss: 0.000000... Val Loss: 0.378811\n",
            "Epoch: 91/100... Step: 68000... Loss: 0.000019... Val Loss: 0.379367\n",
            "Epoch: 92/100... Step: 69000... Loss: 0.000000... Val Loss: 0.379917\n",
            "Epoch: 94/100... Step: 70000... Loss: 0.000000... Val Loss: 0.380233\n",
            "Epoch: 95/100... Step: 71000... Loss: 0.000018... Val Loss: 0.380470\n",
            "Epoch: 96/100... Step: 72000... Loss: 0.000000... Val Loss: 0.380644\n",
            "Epoch: 98/100... Step: 73000... Loss: 0.000000... Val Loss: 0.380723\n",
            "Epoch: 99/100... Step: 74000... Loss: 0.000018... Val Loss: 0.380753\n",
            "Epoch: 100/100... Step: 75000... Loss: 0.000000... Val Loss: 0.380756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = 101\n",
        "# vocab_size = len(word2idx)\n",
        "# hidden_dim = 256\n",
        "# dropout_prob = 0.33\n",
        "# linear_output_dim = 128\n",
        "# output_dim = 9\n",
        "\n",
        "# glove_model = BLSTM_NER_GLOVE(train_embeddings, embedding_dim, linear_output_dim, output_dim, hidden_dim, dropout_prob).to(device)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss( ignore_index = -1)\n",
        "# if torch.cuda.is_available():\n",
        "#   criterion = nn.CrossEntropyLoss( ignore_index = -1).cuda()\n",
        "# glove_optimizer = torch.optim.Adam(glove_model.parameters(), lr=0.1)\n",
        "# # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "# # scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "# # glove_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(glove_optimizer, T_max=1000, eta_min=0.05, last_epoch=-1, verbose=False)\n",
        "# batch_size = 20\n",
        "# # glove_scheduler = ReduceLROnPlateau(glove_optimizer, 'min')\n",
        "\n",
        "# # glove_scheduler = torch.optim.lr_scheduler.OneCycleLR(glove_optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=100)"
      ],
      "metadata": {
        "id": "qARlS2QNhZsr"
      },
      "id": "qARlS2QNhZsr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs = 100\n",
        "# counter = 0\n",
        "# print_every = 1000\n",
        "# clip = 5\n",
        "# valid_loss_min = np.Inf\n",
        "\n",
        "# glove_model.train()\n",
        "# for i in range(epochs):\n",
        "\n",
        "    \n",
        "#     for data, lengths, targets in train_loader:\n",
        "#         counter += 1\n",
        "#         data = data.to(device)\n",
        "        \n",
        "#         targets = targets.to(device)\n",
        "#         # h = tuple([e.data for e in h])\n",
        "#         # inputs, labels = inputs.to(device), labels.to(device)\n",
        "#         glove_model.zero_grad()\n",
        "#         output = glove_model(data.to(device), lengths)\n",
        "\n",
        "#         y_pred_for_loss = output.permute(0,2,1)\n",
        "#         y_pred_for_loss = y_pred_for_loss.to(device)\n",
        "        \n",
        "#         loss = criterion(y_pred_for_loss, targets)\n",
        "#         loss.backward()\n",
        "#         nn.utils.clip_grad_norm_(glove_model.parameters(), clip)\n",
        "#         glove_optimizer.step()\n",
        "       \n",
        "#         if counter%print_every == 0:\n",
        "\n",
        "#           val_losses = []\n",
        "#           glove_model.eval()\n",
        "#           for val_data, val_lengths, val_targets in val_loader:\n",
        "              \n",
        "#               val_data = val_data.to(device)\n",
        "              \n",
        "#               val_targets = val_targets.to(device)\n",
        "#               val_output = glove_model(val_data, val_lengths)\n",
        "#               val_pred_for_loss = val_output.permute(0,2,1)\n",
        "#               val_pred_for_loss = val_pred_for_loss.to(device)\n",
        "#               val_loss = criterion(val_pred_for_loss, val_targets)\n",
        "#               val_losses.append(val_loss.item())\n",
        "              \n",
        "#           glove_model.train()\n",
        "#           print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "#                 \"Step: {}...\".format(counter),\n",
        "#                 \"Loss: {:.6f}...\".format(loss.item()),\n",
        "#                 \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "#                 # if np.mean(val_losses) <= valid_loss_min:\n",
        "#                 #     torch.save(model.state_dict(), './state_dict.pt')\n",
        "#                 #     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "#                 #     valid_loss_min = np.mean(val_losses)\n",
        "#         # glove_scheduler.step()"
      ],
      "metadata": {
        "id": "DvxXqdYrjSp4"
      },
      "id": "DvxXqdYrjSp4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task2model.eval()\n",
        "\n",
        "dev_output = task2model(dev_x_tensor.to(device), dev_lengths)\n",
        "dev_output = dev_output.cpu().detach().numpy()\n",
        "\n",
        "dev_preds = get_preds(dev_output, dev_lengths)\n",
        "write_predictions(\"dev2pred.out\", dev_indices, dev_words, dev_tags, dev_preds, idx2word, idx2tag)"
      ],
      "metadata": {
        "id": "8Z6R5zsnkUkt"
      },
      "id": "8Z6R5zsnkUkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task2model.train()\n",
        "torch.save(task2model.state_dict(), './blstm2.pt')"
      ],
      "metadata": {
        "id": "CFZU-15TNRJb"
      },
      "id": "CFZU-15TNRJb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
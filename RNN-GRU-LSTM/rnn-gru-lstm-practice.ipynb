{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ef0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart_open in /Users/shivani/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (6.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!python3 -m pip install --upgrade smart_open\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c6cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfa14d",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fbb62b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kf/fj8j56v56qs_dw2ggvb7w9y00000gn/T/ipykernel_93911/3862017710.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "# Reading the data from CSV file like in HW 1, skipping any lines that have inconsistent data formats\n",
    "\n",
    "data = pd.read_csv('amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe14b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any rows with null values from the dataframe\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d684e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the data with only the review body and star rating columns\n",
    "\n",
    "data_ = data[['review_body','star_rating']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc5631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into three dataframes by their star rating like in HW 1\n",
    "# - class 1 - (1,2), class 2 - (3), class 3 - (4,5)\n",
    "\n",
    "data_class1 = data_[(data_['star_rating'] == 1) | (data_['star_rating'] == 2)]\n",
    "data_class2 = data_[(data_['star_rating'] == 3)]\n",
    "data_class3 = data_[(data_['star_rating'] == 4) | (data_['star_rating'] == 5)]\n",
    "\n",
    "# randomly sampling 20000 reviews from each class\n",
    "\n",
    "data_class1_sample = data_class1.sample(20000)\n",
    "data_class2_sample = data_class2.sample(20000)\n",
    "data_class3_sample = data_class3.sample(20000)\n",
    "\n",
    "# changing class labels to 0,1,2 for pytorch\n",
    "\n",
    "data_class1_sample['class'] = 0\n",
    "data_class1_sample.drop(columns=['star_rating'], inplace=True)\n",
    "data_class2_sample['class'] = 1\n",
    "data_class2_sample.drop(columns=['star_rating'], inplace=True)\n",
    "data_class3_sample['class'] = 2\n",
    "data_class3_sample.drop(columns=['star_rating'], inplace=True)\n",
    "data_class1_sample.head()\n",
    "\n",
    "# creating final dataframe with 60000 reviews, 20000 from each class\n",
    "\n",
    "final_data_ = pd.concat([data_class1_sample, data_class2_sample, data_class3_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb3e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kf/fj8j56v56qs_dw2ggvb7w9y00000gn/T/ipykernel_93911/1781178430.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(str(text), 'html.parser')\n",
      "/var/folders/kf/fj8j56v56qs_dw2ggvb7w9y00000gn/T/ipykernel_93911/1781178430.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(str(text), 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# functiion to clean data - remove htmls and urls and spaces\n",
    "\n",
    "def clean_data(text):\n",
    "    soup = BeautifulSoup(str(text), 'html.parser')\n",
    "    lowercase = str.lower(soup.get_text())\n",
    "    remove_urls = re.sub(r\"http\\S+\", \"\", lowercase)\n",
    "\n",
    "    remove_spaces = re.sub(' +', ' ', remove_urls)\n",
    "\n",
    "    return remove_spaces\n",
    "\n",
    "# Applying the clean data function to each review in the final data frame\n",
    "# cleaned data is now in 'review_body_cleaned' column\n",
    "\n",
    "final_data_['review_body_cleaned'] = final_data_['review_body'].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628c1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the final data\n",
    "\n",
    "shuffled = final_data_.copy()\n",
    "shuffled = shuffled.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590cf7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "      <th>review_body_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4803375</th>\n",
       "      <td>with 100x accelerator on the bottle i was expe...</td>\n",
       "      <td>1</td>\n",
       "      <td>with 100x accelerator on the bottle i was expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042877</th>\n",
       "      <td>This is a great tan color. It looks really pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>this is a great tan color. it looks really pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284119</th>\n",
       "      <td>The stamper as well as the plates are very har...</td>\n",
       "      <td>0</td>\n",
       "      <td>the stamper as well as the plates are very har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384667</th>\n",
       "      <td>My scalp is of the normal to oily variety. The...</td>\n",
       "      <td>0</td>\n",
       "      <td>my scalp is of the normal to oily variety. the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319583</th>\n",
       "      <td>Its a good starting pallete its not as pigment...</td>\n",
       "      <td>2</td>\n",
       "      <td>its a good starting pallete its not as pigment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  class  \\\n",
       "4803375  with 100x accelerator on the bottle i was expe...      1   \n",
       "2042877  This is a great tan color. It looks really pre...      2   \n",
       "3284119  The stamper as well as the plates are very har...      0   \n",
       "384667   My scalp is of the normal to oily variety. The...      0   \n",
       "3319583  Its a good starting pallete its not as pigment...      2   \n",
       "\n",
       "                                       review_body_cleaned  \n",
       "4803375  with 100x accelerator on the bottle i was expe...  \n",
       "2042877  this is a great tan color. it looks really pre...  \n",
       "3284119  the stamper as well as the plates are very har...  \n",
       "384667   my scalp is of the normal to oily variety. the...  \n",
       "3319583  its a good starting pallete its not as pigment...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330597a",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3307f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc6e20",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a9fb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('colleges', 0.6560817956924438),\n",
       " ('university', 0.6385269165039062),\n",
       " ('school', 0.6081897616386414)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 examples of word similarities using word\n",
    "wv.most_similar(positive=['college'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['king', 'woman'], negative=['queen'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['body'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27587ec",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "462f51f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "reviews = shuffled['review_body_cleaned'].values\n",
    "print(reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e4dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize each of the reviews and storing them in this list\n",
    "reviews_tokenized = []\n",
    "\n",
    "for r in reviews:\n",
    "    tokenized = utils.simple_preprocess(r)\n",
    "    reviews_tokenized.append(tokenized)\n",
    "    \n",
    "# Code framework from https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "# Performing word2vec from gensim module on each tokenized review to generate word embeddings\n",
    "# Use our model to see how similar words are based on their embeddings from word2vec\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in reviews_tokenized:\n",
    "            yield line\n",
    "            \n",
    "sentences = MyCorpus()\n",
    "model = Word2Vec(sentences=sentences, vector_size=300, window=13, min_count=9)\n",
    "model.save(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "236d6ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('law', 0.730804979801178),\n",
       " ('present', 0.7208152413368225),\n",
       " ('mother', 0.7143966555595398)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['college'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d463ec32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 0.555892288684845)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['king', 'woman'], negative=['queen'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba1164c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('healing', 0.6980826258659363)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['sun'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a20635",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a14d22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# What do you conclude from comparing vectors generated by yourself and the pretrained model? \n",
    "# Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "\n",
    "# The vectors created by the the pretrained model and our model both have the same dimensions, N=300, but \n",
    "# both encode semantic similarities differently. The pretrained model does a better job of creating embeddings for \n",
    "# the words than our model based on the comparison of word similarities. Pretrained model associates college with \n",
    "# university and school while our model associates college with transitioning and senior. While our model doesn't associate\n",
    "# college with completely irrelevant words, there is definitely more similarity in the words associated by the \n",
    "# pretrained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090eba4",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb024a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Getting the input features for task 3 - using sklearn perceptron and linearsvm models\n",
    "# For each tokenized review, create an input feature that averages all the embeddings for the tokens in that word, if \n",
    "# that review contains no tokens in the word2vec google vocabulary, then the input feature is just set to zeros\n",
    "\n",
    "input_features = []\n",
    "\n",
    "for review_tokens in reviews_tokenized:\n",
    "    feature = np.zeros(300)\n",
    "    count = 0\n",
    "    for token in review_tokens:\n",
    "        if token in wv:\n",
    "            count += 1\n",
    "            feature += wv[token]\n",
    "    \n",
    "    if count != 0:\n",
    "        feature = feature/float(count)\n",
    "    input_features.append(feature)\n",
    "    \n",
    "input_features = np.array(input_features)\n",
    "print(input_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0070b499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>class</th>\n",
       "      <th>review_body_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4803375</th>\n",
       "      <td>with 100x accelerator on the bottle i was expe...</td>\n",
       "      <td>1</td>\n",
       "      <td>with 100x accelerator on the bottle i was expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042877</th>\n",
       "      <td>This is a great tan color. It looks really pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>this is a great tan color. it looks really pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284119</th>\n",
       "      <td>The stamper as well as the plates are very har...</td>\n",
       "      <td>0</td>\n",
       "      <td>the stamper as well as the plates are very har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384667</th>\n",
       "      <td>My scalp is of the normal to oily variety. The...</td>\n",
       "      <td>0</td>\n",
       "      <td>my scalp is of the normal to oily variety. the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319583</th>\n",
       "      <td>Its a good starting pallete its not as pigment...</td>\n",
       "      <td>2</td>\n",
       "      <td>its a good starting pallete its not as pigment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  class  \\\n",
       "4803375  with 100x accelerator on the bottle i was expe...      1   \n",
       "2042877  This is a great tan color. It looks really pre...      2   \n",
       "3284119  The stamper as well as the plates are very har...      0   \n",
       "384667   My scalp is of the normal to oily variety. The...      0   \n",
       "3319583  Its a good starting pallete its not as pigment...      2   \n",
       "\n",
       "                                       review_body_cleaned  \n",
       "4803375  with 100x accelerator on the bottle i was expe...  \n",
       "2042877  this is a great tan color. it looks really pre...  \n",
       "3284119  the stamper as well as the plates are very har...  \n",
       "384667   my scalp is of the normal to oily variety. the...  \n",
       "3319583  its a good starting pallete its not as pigment...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06a12c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET TRAIN/TEST SPLIT DATA - use 80/20 split \n",
    "\n",
    "ratings = shuffled['class'].values\n",
    "\n",
    "train_x = input_features[0:48000, :]\n",
    "test_x = input_features[48000:, :]\n",
    "\n",
    "train_y = ratings[0:48000]\n",
    "test_y = ratings[48000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cfb6242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.49      0.87      0.63      4020\n",
      "     class 1       0.59      0.36      0.44      4013\n",
      "     class 2       0.84      0.52      0.64      3967\n",
      "\n",
      "    accuracy                           0.58     12000\n",
      "   macro avg       0.64      0.58      0.57     12000\n",
      "weighted avg       0.64      0.58      0.57     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similar to HW1, run perceptron using input_features as input, and the train labels\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "perceptron = Perceptron(penalty=None)\n",
    "perceptron.fit(train_x, train_y)\n",
    "class_names = ['class 0', 'class 1', 'class 2']\n",
    "y_pred_perceptron = perceptron.predict(test_x)\n",
    "\n",
    "print(classification_report(test_y, y_pred_perceptron, target_names=class_names))\n",
    "perceptron_stats = classification_report(test_y, y_pred_perceptron, target_names=class_names, output_dict=True)\n",
    "\n",
    "# The reported accuracy for Perceptron using word2vec input features is 0.58\n",
    "# The reported accuracy for Perceptron using TFIDF (HW1) features is 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c6ded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING STOPWORDS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.65      0.69      0.67      4020\n",
      "     class 2       0.59      0.56      0.57      4013\n",
      "     class 3       0.72      0.72      0.72      3967\n",
      "\n",
      "    accuracy                           0.65     12000\n",
      "   macro avg       0.65      0.65      0.65     12000\n",
      "weighted avg       0.65      0.65      0.65     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similar to HW1, use SVM with input_features as input and train labels\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(multi_class='ovr')\n",
    "svm.fit(train_x, train_y)\n",
    "class_names = ['class 1', 'class 2', 'class 3']\n",
    "y_pred_svm = svm.predict(test_x)\n",
    "print('REMOVING STOPWORDS:\\n')\n",
    "print(classification_report(test_y, y_pred_svm, target_names=class_names))\n",
    "\n",
    "# The reported accuracy for SVM using word2vec input features is 0.65\n",
    "# The reported accuracy for Perceptron using TFIDF (HW1) features is 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f1bce",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1e068ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you conclude from comparing performances for the models trained using the two different feature types \n",
    "# (TF-IDF and your trained Word2Vec features)?\n",
    "\n",
    "# The models performed better on the TFIDF features compared to the word2vec features, this may be because TFIDF\n",
    "# puts an emphasis on words that have a strong association with a sentiment because it has data about how much \n",
    "# words liek that appear in a review, where as for word2vec all the embeddings are averaged, so this may get rid\n",
    "# of information that associates stronger words with a sentiment class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed310b73",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d8597",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ae96424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e99d411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn numpy train and test arrays to tensors\n",
    "\n",
    "train_tensors = torch.from_numpy(train_x)\n",
    "train_labels = torch.from_numpy(train_y)\n",
    "test_tensors = torch.from_numpy(test_x)\n",
    "test_labels = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1e4f75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (l1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (l2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (l3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Code framework for Task 4 from https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "# Define a multilayer perceptron or FNN\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.l1 = nn.Linear(300,100)\n",
    "        self.l2 = nn.Linear(100,10)\n",
    "        self.l3 = nn.Linear(10,3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.l1(x))\n",
    "        x = nn.functional.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and print structure\n",
    "\n",
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fee353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze datasets to return torch.float32 dtype versions of input data\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, train_tensors, train_labels):\n",
    "        self.data = train_tensors.to(torch.float32)\n",
    "        self.labels = train_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "    \n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, test_tensors, test_labels):\n",
    "        self.data = test_tensors.to(torch.float32)\n",
    "        self.labels = test_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc872553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss to CrossEntropyLoss\n",
    "# Initialize optimizer to SGD with learning rate 0.01\n",
    "# set batch size to 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b071fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into train_loader so we can train in batches\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(TrainData(train_tensors, train_labels), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5825e133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 0.818224\n",
      "Epoch: 20 \tTraining Loss: 0.759653\n",
      "Epoch: 30 \tTraining Loss: 0.740917\n",
      "Epoch: 40 \tTraining Loss: 0.725671\n",
      "Epoch: 50 \tTraining Loss: 0.710572\n"
     ]
    }
   ],
   "source": [
    "# We will train model for 50 epochs\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train() \n",
    "    \n",
    "    # Training loop using batches of data from train_loader\n",
    "    # - zero out gradients for optimizer at start of each batch\n",
    "    # - get output\n",
    "    # - calculate loss\n",
    "    # - do a backward step\n",
    "    # - update the gradients based on the backward pass\n",
    "    # - increment the train loss\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    # Get average of train loss\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    if (epoch+1)%10 == 0:\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d69fa23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnvert test tensors to torch.float32 dtype\n",
    "test_tensors = test_tensors.to(torch.float32)\n",
    "\n",
    "# Get predictions\n",
    "preds = model(test_tensors)\n",
    "_, predicted = torch.max(preds, 1) \n",
    "predicted = predicted.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30117ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset using average word2vec vectors:  0.6676666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class_names = ['class 0', 'class 1', 'class 2']\n",
    "print(\"Accuracy on test dataset using average word2vec vectors: \", accuracy_score(test_labels.numpy(), predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7285e65",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "592d380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Get new set of input features by concatenating the word2vec embeddings of first 10 words in a review\n",
    "\n",
    "input_features_concat = []\n",
    "\n",
    "for review_tokens in reviews_tokenized:\n",
    "    feature = []\n",
    "    for token in review_tokens:      \n",
    "        if token in wv:\n",
    "            \n",
    "            feature.append(wv[token])\n",
    "\n",
    "        if len(feature) == 10:\n",
    "            break\n",
    "    \n",
    "    while len(feature) < 10:\n",
    "        feature.append(np.zeros(300, dtype=np.float32))\n",
    "    \n",
    "    feature = np.array(feature)\n",
    "    flattened = feature.flatten()\n",
    "    \n",
    "    input_features_concat.append(flattened)\n",
    "    \n",
    "input_features_concat = np.array(input_features_concat)\n",
    "print(input_features_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfea74d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Get train and test data from new input_features_concat data\n",
    "\n",
    "train_x_concat = input_features_concat[0:48000, :]\n",
    "test_x_concat = input_features_concat[48000:, :]\n",
    "print(train_x_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b0215bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and test data to tensors\n",
    "\n",
    "train_tensors_concat = torch.from_numpy(train_x_concat)\n",
    "test_tensors_concat = torch.from_numpy(test_x_concat)\n",
    "\n",
    "# Load train data\n",
    "train_loader_concat = torch.utils.data.DataLoader(TrainData(train_tensors_concat, train_labels), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3fdbad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as MLP model defined earlier, except. the input layer has size 3000, becuse we are using the first 10 word2vec\n",
    "# embeddings concatenated as input\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.l1 = nn.Linear(3000,100)\n",
    "        self.l2 = nn.Linear(100,10)\n",
    "        self.l3 = nn.Linear(10,3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.l1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = nn.functional.relu(self.l2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "model_b = MLP2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "81b730cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilaize loss, optimizer, and batchsize to 20 again\n",
    "\n",
    "criterion_b = nn.CrossEntropyLoss()\n",
    "optimizer_b = torch.optim.SGD(model_b.parameters(), lr=0.01)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dabac868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 0.870411\n",
      "Epoch: 10 \tTraining Loss: 0.763507\n",
      "Epoch: 15 \tTraining Loss: 0.607733\n",
      "Epoch: 20 \tTraining Loss: 0.436166\n",
      "Epoch: 25 \tTraining Loss: 0.326899\n"
     ]
    }
   ],
   "source": [
    "# We will train MLP2 for 25 epochs\n",
    "n_epochs = 25\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model_b.train() \n",
    "    \n",
    "    # Training loop using batches of data from train_loader\n",
    "    # - zero out gradients for optimizer at start of each batch\n",
    "    # - get output\n",
    "    # - calculate loss\n",
    "    # - do a backward step\n",
    "    # - update the gradients based on the backward pass\n",
    "    # - increment the train loss\n",
    "    for data, target in train_loader_concat:\n",
    "        optimizer_b.zero_grad()\n",
    "        output = model_b(data)\n",
    "        loss_b = criterion_b(output, target)\n",
    "        loss_b.backward()\n",
    "        optimizer_b.step()\n",
    "        train_loss += loss_b.item()*data.size(0)\n",
    "       \n",
    "    # Get average of train loss\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "224720b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset using concat word2vec vectors:  0.5400833333333334\n"
     ]
    }
   ],
   "source": [
    "# COnvert test tensors to torch.float32 dtype\n",
    "test_tensors_concat = test_tensors_concat.to(torch.float32)\n",
    "\n",
    "# Get predictions\n",
    "preds_b = model_b(test_tensors_concat)\n",
    "_, predicted_b = torch.max(preds_b, 1) \n",
    "predicted_b = predicted_b.numpy()\n",
    "\n",
    "class_names = ['class 0', 'class 1', 'class 2']\n",
    "print(\"Accuracy on test dataset using concat word2vec vectors: \", accuracy_score(test_labels.numpy(), predicted_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a696d1",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.\n",
    "\n",
    "# For the FNN:\n",
    "# Accuracy on test dataset using average word2vec vectors:  0.6676666666666666\n",
    "# Accuracy on test dataset using concat word2vec vectors:  0.5400833333333334\n",
    "\n",
    "# To compare with Simple Models:\n",
    "# Accuracy for SVM using word2vec input features is 0.65\n",
    "# Accuracy for Perceptron using word2vec input features is 0.58\n",
    "\n",
    "# The FNN performed better on the average word2vec representations than both the SVM and Perceptron, but worse on the \n",
    "# concatenated vectors than it did on average word2vec. Neural networks learn a lot of features that Perceptron and SVM\n",
    "# may not capture, and this could explain the better performance. Also, with more training, its possible that the performance\n",
    "# could still improve.\n",
    "# We might need to tune the hyperparameters for the FNN on the concatenated vectors to improve its performance \n",
    "# because the concatenated vectors are a much different representation of a review than the average vectors are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d66bc",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac52c3",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bb66ad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 20, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "# Function to convert a review to a tensor of size [20,1,300], 20 is the number of words we are considering\n",
    "# and [1,300] is the shaoe of the word2vec vector associated with each word\n",
    "\n",
    "def reviewToTensor(review_tokenized):\n",
    "    tensor = np.zeros((20, 1, 300))\n",
    "    \n",
    "    count = 0\n",
    "    for word in review_tokenized:\n",
    "        if word in wv:\n",
    "            tensor[count][0] = wv[word]\n",
    "            count += 1\n",
    "        \n",
    "        if count == 20:\n",
    "            break\n",
    "        \n",
    "    while count < 20:\n",
    "        tensor[count][0] = np.zeros((1,300))\n",
    "        count += 1\n",
    "        \n",
    "    return tensor\n",
    "\n",
    "# Create tensored_reviews, new dataset for task 5\n",
    "\n",
    "tensored_reviews = []\n",
    "\n",
    "for review in reviews_tokenized:\n",
    "    tensored_reviews.append(reviewToTensor(review))\n",
    "    \n",
    "tensored_reviews = np.array(tensored_reviews, dtype=np.float32)\n",
    "tensored_reviews = torch.from_numpy(tensored_reviews, )\n",
    "print(tensored_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "88c6a507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48000, 300])\n",
      "torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "# Get train and test data from tensored_reviews\n",
    "\n",
    "train_tensors_rnn = tensored_reviews[0:48000]\n",
    "test_tensors_rnn = tensored_reviews[48000:]\n",
    "print(train_tensors.shape)\n",
    "print(train_tensors_rnn[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9e44d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model using code framework from https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.sigmoid(self.i2h(combined))\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # trying a new hidden initialization\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
    "\n",
    "# Number of hidden states = 20\n",
    "# Number of classes = 3\n",
    "# Input size is size of word2vec embedding = 300\n",
    "n_hidden = 20\n",
    "n_categories = 3\n",
    "input_size = 300\n",
    "# Initialize rnn model\n",
    "rnn = RNN(input_size, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ccc34412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss, optimizer, and learning rate for rnn model \n",
    "\n",
    "criterion_rnn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer_rnn = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "de4b2716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.111851 \tTime: 2m 10s\n",
      "Epoch: 2 \tTraining Loss: 1.106599 \tTime: 4m 18s\n",
      "Epoch: 3 \tTraining Loss: 1.090781 \tTime: 6m 27s\n",
      "Epoch: 4 \tTraining Loss: 1.087644 \tTime: 8m 40s\n",
      "Epoch: 5 \tTraining Loss: 1.086765 \tTime: 10m 50s\n",
      "Epoch: 6 \tTraining Loss: 1.074962 \tTime: 12m 58s\n",
      "Epoch: 7 \tTraining Loss: 1.070315 \tTime: 15m 18s\n",
      "Epoch: 8 \tTraining Loss: 1.064982 \tTime: 17m 26s\n",
      "Epoch: 9 \tTraining Loss: 1.062178 \tTime: 19m 40s\n",
      "Epoch: 10 \tTraining Loss: 1.056471 \tTime: 21m 55s\n",
      "Epoch: 11 \tTraining Loss: 1.054779 \tTime: 24m 2s\n",
      "Epoch: 12 \tTraining Loss: 1.055611 \tTime: 26m 48s\n",
      "Epoch: 13 \tTraining Loss: 1.051930 \tTime: 29m 9s\n",
      "Epoch: 14 \tTraining Loss: 1.043313 \tTime: 31m 18s\n",
      "Epoch: 15 \tTraining Loss: 1.044487 \tTime: 33m 21s\n",
      "Epoch: 16 \tTraining Loss: 1.038194 \tTime: 35m 19s\n",
      "Epoch: 17 \tTraining Loss: 1.044760 \tTime: 37m 20s\n",
      "Epoch: 18 \tTraining Loss: 1.042654 \tTime: 39m 20s\n",
      "Epoch: 19 \tTraining Loss: 1.043511 \tTime: 41m 19s\n",
      "Epoch: 20 \tTraining Loss: 1.034712 \tTime: 43m 36s\n",
      "Epoch: 21 \tTraining Loss: 1.035747 \tTime: 46m 20s\n",
      "Epoch: 22 \tTraining Loss: 1.039096 \tTime: 48m 40s\n",
      "Epoch: 23 \tTraining Loss: 1.027214 \tTime: 50m 48s\n",
      "Epoch: 24 \tTraining Loss: 1.030628 \tTime: 55m 1s\n",
      "Epoch: 25 \tTraining Loss: 1.024800 \tTime: 57m 59s\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_iters = 48000\n",
    "n_epochs = 25\n",
    "\n",
    "\n",
    "# RNN training loop\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "        # get random training sample \n",
    "        ind = np.random.randint(low=0, high=48000)\n",
    "        \n",
    "        tensored_review = train_tensors_rnn[ind]\n",
    "        target = train_labels[ind]\n",
    "        \n",
    "        # initialize hidden state and optimizer\n",
    "        hidden = rnn.initHidden()\n",
    "        optimizer_rnn.zero_grad()\n",
    "        \n",
    "        # get output for this review\n",
    "        for i in range(20):\n",
    "            output, hidden = rnn(tensored_review[i], hidden)\n",
    "            \n",
    "        # get loss, and perform a backward pass, update model params\n",
    "        loss = criterion_rnn(output, torch.unsqueeze(target,0))\n",
    "        loss.backward()\n",
    "        # added this line to clip gradients \n",
    "        nn.utils.clip_grad_norm_(rnn.parameters(), 1)\n",
    "        optimizer_rnn.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss/n_iters\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "25825d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate rnn model on test dataset\n",
    "rnn.eval()\n",
    "\n",
    "predictions_rnn = []\n",
    "\n",
    "for i, tensored_review in enumerate(test_tensors_rnn):\n",
    "    label = test_labels[i]\n",
    "    hidden_vec = rnn.initHidden()\n",
    "    \n",
    "    # make predictions on each review in test dataset\n",
    "    for i in range(20):\n",
    "        output, hidden_vec = rnn(tensored_review[i], hidden_vec)\n",
    "        \n",
    "    _, predicted_rnn = torch.max(output, 1) \n",
    "    predictions_rnn.append(predicted_rnn.numpy()[0])\n",
    "    \n",
    "predictions_rnn = np.array(predictions_rnn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "99a4dd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a0d9d1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset with RNN using word2vec vectors:  0.6004166666666667\n"
     ]
    }
   ],
   "source": [
    "class_names = ['class 0', 'class 1', 'class 2']\n",
    "print(\"Accuracy on test dataset with RNN using word2vec vectors: \", accuracy_score(test_labels.numpy(), predictions_rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952540df",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.\n",
    "\n",
    "# Accuracy on test dataset with RNN using word2vec vectors:  0.6004166666666667\n",
    "\n",
    "# This accuracy was a result of using a learning_rate of 0.001 which may have been two small, and running only 25 epochs\n",
    "# Its possible that the rnn model would still improve if trained for more epochs, and may have converged faster\n",
    "# with a slightly larger learning rate. The performance of the RNN on the input data using word2vec is worse than FNN\n",
    "# on the average vectors but better than FNN on the 10 first concatenated vectors. \n",
    "# RNN uses the concept of history using the hidden states to generate the next hidden states and predictions, and this \n",
    "# functionality makes RNN better dealing with sequences as compared to an FNN. \n",
    "# A problem RNN could be facing is vanishing gradients, which means the weights may not be updating by much at all which\n",
    "# leads to the model barely learning. GRU and LSTM should have better performance than simple RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4ebc244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code frameworks and ideas from:\n",
    "# https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "# https://jaketae.github.io/study/pytorch-rnn/\n",
    "\n",
    "# Initialize GRU model class\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # Added gru layer with input_size = 300\n",
    "        self.gru = nn.GRU(input_size=300, hidden_size=hidden_size, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.linear(output[-1])\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # trying a new hidden initialization\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))\n",
    "\n",
    "n_hidden = 20\n",
    "n_categories = 3\n",
    "\n",
    "# Initialize gru model\n",
    "gru = GRU(n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ab020c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_gru = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer_gru = torch.optim.Adam(gru.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b1564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.035188\n",
      "Epoch: 2 \tTraining Loss: 1.049782\n",
      "Epoch: 3 \tTraining Loss: 1.053069\n",
      "Epoch: 4 \tTraining Loss: 1.051512\n"
     ]
    }
   ],
   "source": [
    "n_iters = 48000\n",
    "n_epochs = 25\n",
    "\n",
    "# Same training loop as RNN task 5a)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "        ind = np.random.randint(low=0, high=48000)\n",
    "        \n",
    "        tensored_review = train_tensors_rnn[ind]\n",
    "        target = train_labels[ind]\n",
    "        \n",
    "        hidden = gru.initHidden()\n",
    "        optimizer_gru.zero_grad()\n",
    "        \n",
    "        for i in range(20):\n",
    "            output, hidden = gru(tensored_review[i], hidden)\n",
    "\n",
    "        loss = criterion_gru(output, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(gru.parameters(), 1)\n",
    "        optimizer_gru.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss/n_iters\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d7109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
